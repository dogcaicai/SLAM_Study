传感器的融合
============

单一的传感器功能往往十分有限，但是多个传感器之间的相互关系复杂，很难直接描述。
现在可以利用神经网络直接连接各个传感器，并且通过神经网络的网络的拓扑结构来表达之间的相互关系。来解决各种滤波的问题。

每一种传感器都有其独特的优势与弱点。可以通过组合来实现取长取补短。同时处理复杂环境数据，会造成车辆驾驶决策的延迟，而最大限度减少这种延迟，是提升车辆安全性的又一关键部分，一个方案那就是数据融合能够减少系统做驾驶决策所需的计算资源。

例如无人驾驶通过传感器融合技术，将各种传感器的不同视角整合，并最终赋能自动驾驶车辆环境感知。 并且通过冗余来提供鲁棒性。  例如毫米波雷达可在低分辨率情况下完成测距，且受天气困素影响很小，而摄像头有更高的分辨率，能够感知颜色，但受强光影响较大，激光雷达则能够提供三维尺度感知信息，对环境的重构能力更强。


同时利用人工智能自动不断迭代来建立一个高精度的地图。

自动驾驶对于计算量的要求。

现在的奥迪A8可以在60KM/h下实现自动驾驶，你就是16.66m/s. 每秒20fps,其核心是每帧可以前进行多少米。也就是16.66/20=0.833 也就是计算每帧0.833秒。 这个同时也与探测的安全的距离相关，也就是给你多少反应时间。同时这么大的计算量的对于资源的消耗又是怎样的。




相机
====

相机的模型，那就要做相机的标定，包括内参数与外参数。
内参数就是光学参数，焦距以及畸变参数。
而外参数则是指相机的位姿，相比于不变的内参，外参数会随着相机的运动发生改变，同时


ORB
===

orb 加了码盘信息(帧间约束)，走2.4米，跟踪点少，光照100流明以下时，误差30～10cm
特征点跟踪100个以下时，误差太。

普通摄像头噪点多，误匹配多，系统极不稳定。


现在能否直接把识别直接放在传感器，对了用FPGA。把FPGA + DL + 传感器放在传感器上。



SAR
===
AR利用的是全息成像的原理来提高分辨率. SAR要垂直目标运动， 汽车不合适。

雷达
====

要穿透粉尘和烟雾，波长更短的波不是表现效果更好么 
常规雷达波的波长比红外还长，穿透力还不如可见光 
这岂不是从一开头就背道而驰 
奇了怪了，可能这方面我的背景知识不够吧 
穿透的话常规都是想的伽马射线和X射线短波长连人体都穿透摄像 
理论上短波长才是透射强啊 
地铁安检用的也是X光机穿透皮包 
所以这个。。 
唉 
光学没专门学过不懂了 
怎么就可见光谱不能穿透烟雾呢 
不懂不懂，一堆程序猿讨论光学知识 
我觉得我们这里缺了光学专业的大佬 
电磁波具有绕射能力，波长越短，绕射能力越差 
@125.60 你可以关注一下这个公司，看有没有合适的产品 http://www.360doc.com/content/17/0228/18/8507568_632753112.shtml 
太赫兹安检主要是中电和中航在做，大学和民企连仪器都买不起，也拿不到国家补贴 /
