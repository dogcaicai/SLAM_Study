传感器的融合
============

单一的传感器功能往往十分有限，但是多个传感器之间的相互关系复杂，很难直接描述。
现在可以利用神经网络直接连接各个传感器，并且通过神经网络的网络的拓扑结构来表达之间的相互关系。来解决各种滤波的问题。

每一种传感器都有其独特的优势与弱点。可以通过组合来实现取长取补短。同时处理复杂环境数据，会造成车辆驾驶决策的延迟，而最大限度减少这种延迟，是提升车辆安全性的又一关键部分，一个方案那就是数据融合能够减少系统做驾驶决策所需的计算资源。

例如无人驾驶通过传感器融合技术，将各种传感器的不同视角整合，并最终赋能自动驾驶车辆环境感知。 并且通过冗余来提供鲁棒性。  例如毫米波雷达可在低分辨率情况下完成测距，且受天气困素影响很小，而摄像头有更高的分辨率，能够感知颜色，但受强光影响较大，激光雷达则能够提供三维尺度感知信息，对环境的重构能力更强。


同时利用人工智能自动不断迭代来建立一个高精度的地图。

自动驾驶对于计算量的要求。

现在的奥迪A8可以在60KM/h下实现自动驾驶，你就是16.66m/s. 每秒20fps,其核心是每帧可以前进行多少米。也就是16.66/20=0.833 也就是计算每帧0.833秒。 这个同时也与探测的安全的距离相关，也就是给你多少反应时间。同时这么大的计算量的对于资源的消耗又是怎样的。




相机
====

相机的模型，那就要做相机的标定，包括内参数与外参数。
内参数就是光学参数，焦距以及畸变参数。
而外参数则是指相机的位姿，相比于不变的内参，外参数会随着相机的运动发生改变，同时SLAM中待估计的目标。


