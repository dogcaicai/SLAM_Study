#############
SLAM 算法分析 
#############


.. include:: /Stage_3/SLAM_book.rst
.. include:: /Stage_3/DL_SLAM.rst

#. 各个算法分析与对比



双目相机模型
============

利用视差来计算深度，并且 视差与距离成反比。虽然视差计算深度的公式很简单，但是视差本身的计算比较困难。
需要左右眼图像点一一匹配关系，如果计算每个像素的的深度时，其计算量与精度将成为问题，而且只有图像毋庸纹理
变化丰富的地方才能计算视差，计算还是GPU或者FPGA来计算。

RGBD-SLAMV2
-----------

它可以直接主动测每个像素的深度，现在发为两种

#. 通过红外结构光
#. 通过 *飞行时间法* 来测量。

而采用RGB-D 则生成采集到点云。

采用DVO的方法，直接上PCL来做了。

SLAM++

LSD-SLAM
--------

LSD-SLAM is a novel approach to real-time monocular SLAM. It is fully direct (i.e. does not use keypoints / features) and creates large-scale, semi-dense maps in real-time on a laptop。
是直接法最好的SLAM,就要求传感器有深度信息,如果有每一点深度信息那叫Dense depth maps. Semi-dense Depth maps就是深度图并不并包含每一个象素点,只有移动像素的深度图.如果这些移动的点是刚体运动,这样包含了 rigid body motion+ scale.
https://fradelg.gitbooks.io/real-time-3d-reconstruction-from-monocular-video/content/stereo/semi-dense.html
原理: 主要是解决地图的3D重构,直接三维的点来恢复出3D结构,例如深度图,然后用点云匹配生成三维结构.这里dense map,其实相当于图形的Depth buffer,以及Z-buffer的做法.

.. math:: 

   K_{i} = ( I_{i}, D_{i},V_{i})

其中每一帧包含三大块，亮度值，深度值，以及深度方差值。

Steps:
#. 参考帧上极线的计算。
#. 极线上得到最好的匹配位置
#. 通过匹配位置计算出最佳的深度。

其实上面最终都是误差的计算，
#. 几何差异误差，原于相对朝向以及投影矩阵。
#. 图像差异误差，
#. 基线量化误差

深度估计直接使用，直接采用卡尔曼滤波来实现。要通过profiling来快速得到其workflow.
然后结合callgraph. 三角化之后，就可以充分利用Opengl来直接化了。

视频:https://www.youtube.com/watch?v=GnuQzP3gty4&feature=youtu.be&t=22s
code: github.com/tum-vision/lsd_slam
代码解读:http://blog.csdn.net/lancelot_vim/article/details/51708412
原理: http://vision.in.tum.de/research/vslam/lsdslam
优点: 不需要keypoints,features 提取,精度高,尤其是在室内这种环境.
主要是采用keyframe的方式，进行，有没有办法用skybox再加上LOD来生成一个keyframe. 一个keyframe其实是
生成两个双目frame的视频流。如何反向呢。
性能:

..  slamcn.org 用户名victor 密码是victor@slamcn 邮箱是gwli@outlook.com

SVO
---

性能:近年来非常火的SLAM路线，能恢复半稠密三维，但目前的实现不太好，效率也不高
但是完全可以用OPENGL+CUDA 的汇合来实现同时在cuda中再加入DL.
LSD-SLAM就是SVO的一种
主要步骤
#. 稀疏直接法， 其实就是金字塔图形里找
#. 重投影，把关键点重新投影
#. 优化
#. 重定位


ORB-SLAM
---------

是目前基于特征的单目SLAM系统中效果最好的。
基于ORB特征来实现环境一致的描述。利用orb贯穿全程，并且dbow2库来进行回环检测。采用nearest neighbor方法训练出来的140多m的树状词典
所有优化问题全是基于g2o算，特征匹配直接用ORB的描述符，回环检测用BOW。并没有太多学术上的创新，论文里几乎没有什么公式。

原理: 

#. MapPoints->KeyFrames->CoovisibilityGraph->SpanningTree 具体的流程可以参考 SLAM、orb-slam7.1 简单重构泡泡.pptx
#. 并且采用了面向对象思想封装,把获取的图像数据装成帧。后续处理都是针对帧处理。

   .. code-block:: c 

      Frame {
         Image-> 存储图像数据(输入)
         Camera-> 存储相机的内参信息(输入)
         Feature-> 存储帧对应的特征数据（计算） 一对多的关系
         Mat -> 存储帧对应相机的姿态 (计算)
      }
      Keyframe extends Frame;
      Feature {
      }
      MapPoints {
      }
      是不是可以相当于骨骼动化的关系。

#.  Trac根据运动模型，可以opengl中的T&L变换，然后进行对比。

orb-slam得到在世界坐标系中位置 值很小 很多都是0.00几 
orb-slam2中 输出的位姿 乘以初始化成功时该帧的变换矩阵 所得变换矩阵是世界坐标系到相机坐标系的转换么 SLAM-职业交流群 无题
李
code: https://github.com/raulmur/ORB_SLAM2
优点: 特征提取的匹配的计算量减少，回环检测做很好，接口丰富,对于地图密度要求不高的定位和追踪问题，ORB－SLAM是个不错的选择
缺点:  
#. kinect2 qhd分辨率下（960x540），默认参数，thinkpad T450，帧率<=10Hz 计算量具大。
#. 运行前要读取一个几百兆的字典——调试程序的时候比较考验耐心， 巨大的词典。目前orb使用了一个用nearest neighbor方法训练出来的140多m的树状词典。前面已经说过，这个词典在整个orb中至关重要，每次运行前必须载入至内存。在安卓手机上载入的过程长达三分钟，很恼人
#. 比较容易lost，虽然也容易找回来；
#. orb 所建图非常稀疏，而稀疏的地图对于机器人下一步的应用会造成很大困难
#. 系统中有很多magic number，比如特征匹配的阈值，回环图像对比的阈值，都是经验设定，在不同场景下对应值也有所不同。这些数其实可以通过机器学习的方法学习得到
#. 采用keyframe这样的空间是不连续的。

性能:  
#. i7,640*480的图像中提取500orb约用时13ms,匹配精度可以接受，满足实时性要求
#. 追踪部分，平均每帧约30毫秒，基本达到了30fps。特征提取速度是非常快的，平均11毫秒左右，非常适合于实时SLAM。姿态估计稍微耗时一些，平均需要20毫秒，特别是姿态优化需要耗费16毫秒的时间。
#. 地图构建部分，平均每关键帧约385毫秒。其中生成新的点约70毫秒，Local BA约300毫秒，相对还是比较耗时的。不知道这两部分还有没有优化的空间。

#. 关键点的提取与描述子的计算非常耗时。实践当中，SIFT目前在CPU上是无法实时计算的，而ORB也需要近20毫秒的计算。如果整个SLAM以30毫秒/帧的速度运行，那么一大半时间都花在计算特征点上


DSO
====


发现DSO方法的特征点提取比ORB更耗时 .
SVO,

DTAM
Dense Planar SLAM,
OpenRatSLAM,
G2O,
SBA,
iSAM,
GPU-SLAM

#. 什么时候开始的
#. 每种技术优缺点

主要研究内容
============

#. 传感器的鲁棒性以及传感器融合。
1. 精度上，AR一般更关注于局部精度，要求恢复的相机运动避免出现漂移、抖动，这样叠加的虚拟物体才能看起来与现实场景真实地融合在一起;机器人一般更关注全局精度，需要恢复的整条运动轨迹误差累积不能太大，循环回路要能闭合，而在某个局部的漂移、 抖动等问题往往对机器人应用来说影响不大。

#. 复杂环境的场景理解
前面的都是基础的SLAM，只有"定位"和"建图"两件事。这两件事在今天已经做的比较完善了。近几年的RGB-D SLAM[5], SVO[6], Kinect Fusion[7]等等，都已经做出了十分炫的效果。但是SLAM还未走进人们的实际生活。为什么呢？　　因为实际环境往往非常复杂。灯光会变，太阳东升西落，不断的有人从门里面进进出出，并不是一间安安静静的空屋子，让一个机器人以2cm/s的速度慢慢逛。论文中看起来酷炫的算法，在实际环境中往往捉襟见肘，处处碰壁。向实际环境挑战，是SLAM技术的主要发展方向，也就是我们所说的高级话题。主要有：动态场景、语义地图、多机器人协作等等。
http://www.cnblogs.com/gaoxiang12/p/4395446.html
非特点SLAM， 动态环境，多机器人协作，长时间SLAM。 语义信息。 拓扑/网格地图。
#. 复杂背景下慢速的目标的识别,(怎样识别，识别哪些？)
#. 动态场景运动分离技术
#. 主动SLAM,SLAM+ Exploration.


#. SLAM 的计算需求巨大的，现有算法达不到实时性的要求。
效率上，AR需要在有限的计算资源下实时求解，人眼的刷新率为24帧，所以AR的计算效率通常需要到达30帧以上,但是要想流畅就得每秒60fps 也就是小于16ms,如果想没有眩晕，要至少达到90fps也就是要11.111111ms.; 机器人本身运动就很慢，可以把帧率降低，所以对算法效率的要求相对较低。
3. 配置上，AR对硬件的体积、功率、成本等问题比机器人更敏感，比如机器人上可以配置鱼眼、双目或深度摄像头、高性能CPU等硬件来降低SLAM的难度，而AR应用更倾向于采用更为高效、鲁邦的算法达到需求。

#. 研究利用传感器神经网络与回声状态状态网络 解决鲁棒性与精度的问题。
#. 研究利用3D重构技术来实现地图的重建与深度学习来解决复杂场景理解。
#. 研究利用LLVM来实现全时优化来解决算法的效率，利用GPU与FGPA来提供的计算能力。

由于SLAM 算法的本身的复杂性，以及硬件平台的多样性，开发难度大，需要直接使用现有大量的成熟的库来加快开发速度。例如对于eigen3矩阵运算库，Ceres 优化库，以及OpenCV,PCL等大量库使用。可以提高开效率。但是对运行效率很难保证，实时性更无从谈起。更加大的算法仿真的效率。需要用大量的分析优化工作来运行效率。
LLVM是基于SSA的能够提供编译时期，链接时期，运行时期，闲置时期全时优化的动态编译技术。利用利用LLVM 的SSA形式的IR来分析数据依赖关系，删除无效代码，实现循环的展开，尽可能并行化，根据硬件资源特性与限制最大化利用硬件资源例如寄存器与高速的cache,ARM的NOEN指令，以及GPU的SIMT(Single instruction Multiplethread)技术。大大提高算法的计算效率。同时利用LLVM本身指令信息的完备性，能够对于进行硬件模型的精确化分析。
#. GPU加速的SLAM

